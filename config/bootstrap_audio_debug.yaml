##
# Bootstrap audio-debug (single reproducible config)
#
# This is the *one* config we used to reproduce:
#   outputs/output_diffusion_bootstrap_audio_debug_6s.mp4
#
# Typical usage:
# - Train diffusion (fast bootstrap) and then infer 6s demo in one command:
#     uv run python run_pipeline.py --override-config config/bootstrap_audio_debug.yaml --force-diffusion
#
# Notes:
# - Uses debug visualizers only (data.audio_debug_mode) so the mapping is learnable.
# - Uses silence/dropout audio so “settle during silence” is human-auditable.
# - Uses direct shuffle-pressure so ignoring audio becomes expensive.
# - Disables per-clip feature normalization to preserve absolute loudness/silence cues.
##

data:
  # Inference demo length (training uses same duration; keep reasonable for speed)
  duration: 6.0
  height: 128
  width: 128
  fps: 15

  # Bootstrap distribution
  audio_debug_mode: true
  audio_silence_mix_prob: 1.0

model:
  frame_size: [128, 128]
  # duration * fps = 90
  num_frames: 90
  d_model: 256

diffusion:
  # Training (fast-ish bootstrap)
  epochs: 3
  samples_per_epoch: 500
  batch_size: 16
  timesteps: 15
  frames_per_clip: 10

  # Conditioning
  audio_feature_context: 1
  audio_feature_normalize_per_clip: false

  # Temporal (keep mild)
  seq_len: 12
  prev_latent_weight: 0.5

  # Enable audio CFG
  p_audio_drop: 0.2
  # Remove style channel
  p_style_drop: 1.0

  # Direct shuffle pressure
  audio_shuffle_loss_weight: 2.0
  audio_shuffle_loss_margin: 0.05
  audio_shuffle_loss_prob: 0.75

tracker:
  enabled: false

inference:
  output_video: "outputs/output_diffusion_bootstrap_audio_debug_6s.mp4"
  style_mode: "zero"
  # Push audio dependence to be visually obvious
  audio_guidance_scale: 6.0

