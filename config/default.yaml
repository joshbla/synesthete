project:
  name: "synesthete"
  version: "0.1.0"

data:
  train_dir: "data/train"
  num_samples: 100 # Scaled up
  sample_rate: 16000
  duration: 3.0
  height: 256
  width: 256
  fps: 30

model:
  frame_size: [256, 256]
  # 3.0 seconds * 30 fps = 90 frames
  num_frames: 90
  latent_dim: 256
  d_model: 512
  
train:
  output_dir: "models"
  epochs: 40
  batch_size: 32
  learning_rate: 0.0001
  save_name: "model_latest.pth"
  samples_per_epoch: 200 # Scaled up
  
vae:
  epochs: 20
  samples_per_epoch: 200 # Scaled up
  learning_rate: 0.0001
  batch_size: 64
  kld_weight: 0.1

diffusion:
  epochs: 40
  samples_per_epoch: 200 # Scaled up
  learning_rate: 0.0001
  batch_size: 32
  timesteps: 50
  # Phase 4 temporal coherence
  # Train on contiguous frame snippets and condition each frame on the previous latent.
  #
  # - seq_len: how many contiguous frames are sampled per synthetic clip during diffusion training.
  #   Higher = stronger temporal signal (usually less flicker), but slower training and can over-regularize.
  #   Typical values: 4–24. Minimum is 2.
  seq_len: 8
  #
  # - prev_latent_weight: how strongly the model uses the previous-frame latent as conditioning.
  #   0.0 disables temporal coupling (reverts to i.i.d.-ish frames); 1.0 is the default.
  #   Try 0.5 for weaker smoothing; 2.0–3.0 for stronger coherence (may get “stuck”/over-smooth).
  prev_latent_weight: 1.0
  #
  # Training robustness knobs (reduce train/infer mismatch for prev_latent):
  # At inference, prev_latent comes from the model’s own previous prediction, not ground truth.
  # These knobs make the model robust to imperfect / noisy prev_latent inputs.
  #
  # - p_prev_latent_drop: with this probability, drop prev_latent conditioning entirely (set to 0).
  #   Useful if the model over-relies on temporal smoothing and ignores audio.
  #   Try: 0.0–0.3
  p_prev_latent_drop: 0.1
  #
  # - p_prev_latent_noisy: with this probability, replace prev_latent with a version noised to the SAME diffusion timestep t.
  #   This helps because during denoising, the model sees noisy latents, not clean ones.
  #   Try: 0.0–1.0 (a common starting point is 0.5)
  p_prev_latent_noisy: 0.5
  # Phase 2 factorization (continuous style latent)
  style_dim: 64
  p_style_drop: 0.2
  # Phase 1 conditioning (frame-aligned audio feature timeline)
  # Each target frame is conditioned on its aligned audio feature vector, optionally
  # with a small neighbor context window (audio_feature_context).
  audio_feature_n_fft: 512
  audio_feature_num_bands: 8
  audio_feature_context: 0
  frames_per_clip: 10

tracker:
  enabled: true
  entity: null # Set this if using a team, otherwise defaults to user
  log_videos: false # Disable video uploads to save data/storage


inference:
  output_video: "output_test.mp4"
  # Phase 2: style control at inference time.
  # - "random": sample a new style latent per run (default)
  # - "zero": disable style randomness (useful for smoke tests / audio-reactivity checks)
  style_mode: "random"
  # Optional integer seed for deterministic style sampling (null = no seeding).
  style_seed: null

# Phase 3 data diversity knobs (optional).
# These affect which procedural visualizers are sampled for synthetic training data.
visualizers:
  # Probability of using an augmented wrapper (smooth postprocess stack).
  aug_prob: 0.35
  # Probability of choosing an organic/painterly family vs geometric.
  organic_prob: 0.5
  # Probability of composing 2–3 primitives into a single clip.
  composite_prob: 0.25
