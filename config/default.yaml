project:
  name: "synesthete"
  version: "0.1.0"

data:
  train_dir: "data/train"
  num_samples: 100 # Scaled up
  sample_rate: 16000
  duration: 3.0
  height: 256
  width: 256
  fps: 30
  # Phase 5 helper: optional training distribution to prove audio dependence.
  # When enabled, synthetic training clips come from a small set of deterministic,
  # strongly audio-reactive debug programs (no augmentation/composition).
  audio_debug_mode: false
  # If > 0, mix debug programs into normal sampling with this probability.
  # Useful for gradually reintroducing diversity (e.g., 0.2–0.5).
  audio_debug_mix_prob: 0.0
  # Phase 5 human-eval helper: mix in clips with explicit silences/track dropouts.
  # These make it obvious when visuals should settle (silence) and which elements should respond.
  audio_silence_mix_prob: 0.0

model:
  frame_size: [256, 256]
  # 3.0 seconds * 30 fps = 90 frames
  num_frames: 90
  latent_dim: 256
  d_model: 512
  
train:
  output_dir: "models"
  epochs: 40
  batch_size: 32
  learning_rate: 0.0001
  save_name: "model_latest.pth"
  samples_per_epoch: 200 # Scaled up
  
vae:
  epochs: 20
  samples_per_epoch: 200 # Scaled up
  learning_rate: 0.0001
  batch_size: 64
  kld_weight: 0.1

diffusion:
  epochs: 40
  samples_per_epoch: 200 # Scaled up
  learning_rate: 0.0001
  batch_size: 32
  timesteps: 50
  # Phase 4 temporal coherence
  # Train on contiguous frame snippets and condition each frame on the previous latent.
  #
  # - seq_len: how many contiguous frames are sampled per synthetic clip during diffusion training.
  #   Higher = stronger temporal signal (usually less flicker), but slower training and can over-regularize.
  #   Typical values: 4–24. Minimum is 2.
  seq_len: 8
  #
  # - prev_latent_weight: how strongly the model uses the previous-frame latent as conditioning.
  #   0.0 disables temporal coupling (reverts to i.i.d.-ish frames); 1.0 is the default.
  #   Try 0.5 for weaker smoothing; 2.0–3.0 for stronger coherence (may get “stuck”/over-smooth).
  prev_latent_weight: 1.0
  #
  # Training robustness knobs (reduce train/infer mismatch for prev_latent):
  # At inference, prev_latent comes from the model’s own previous prediction, not ground truth.
  # These knobs make the model robust to imperfect / noisy prev_latent inputs.
  #
  # - p_prev_latent_drop: with this probability, drop prev_latent conditioning entirely (set to 0).
  #   Useful if the model over-relies on temporal smoothing and ignores audio.
  #   Try: 0.0–0.3
  p_prev_latent_drop: 0.1
  #
  # - p_prev_latent_noisy: with this probability, replace prev_latent with a version noised to the SAME diffusion timestep t.
  #   This helps because during denoising, the model sees noisy latents, not clean ones.
  #   Try: 0.0–1.0 (a common starting point is 0.5)
  p_prev_latent_noisy: 0.5
  # Phase 2 factorization (continuous style latent)
  style_dim: 64
  p_style_drop: 0.2
  # Phase 1 conditioning (frame-aligned audio feature timeline)
  # Each target frame is conditioned on its aligned audio feature vector, optionally
  # with a small neighbor context window (audio_feature_context).
  audio_feature_n_fft: 512
  audio_feature_num_bands: 8
  audio_feature_context: 0
  # If true, z-score features per clip. For silence/settling tests, setting this false
  # preserves absolute loudness cues (especially when combined with abs_rms channel).
  audio_feature_normalize_per_clip: true
  # Phase 5: richer features for clearer responsiveness (optional)
  audio_feature_include_abs_rms: true
  audio_feature_include_onset: true
  audio_feature_include_flux_bands: false
  frames_per_clip: 10

  # Phase 5 (optional): make "ignoring audio" punishable
  #
  # This adds a small learned matcher/discriminator that tries to tell if (audio, latent)
  # pairs are matched vs mismatched. Its score can be used as an auxiliary training signal
  # so diffusion is pressured to actually use audio.
  #
  # - match_enabled: enables matcher training + generator-side penalty
  # - match_weight: scale of the generator penalty (higher = stronger push to use audio)
  # - match_lr: learning rate for the matcher network
  # - match_d_model: internal width of matcher
  # - match_dropout: dropout inside matcher MLPs
  match_enabled: false
  match_weight: 0.1
  match_lr: 0.0001
  match_d_model: 256
  match_dropout: 0.0
  # Phase 5: classifier-free audio conditioning (train-time).
  # With this probability, replace audio conditioning with zeros so the model learns an unconditional path.
  # This enables audio guidance at inference time via inference.audio_guidance_scale.
  p_audio_drop: 0.1

  # Phase 5: direct audio-shuffle pressure (no extra network).
  # Adds a hinge penalty encouraging the model to do worse when audio conditioning is shuffled.
  audio_shuffle_loss_weight: 0.0
  audio_shuffle_loss_margin: 0.05
  audio_shuffle_loss_prob: 0.5

tracker:
  enabled: true
  entity: null # Set this if using a team, otherwise defaults to user
  log_videos: false # Disable video uploads to save data/storage


inference:
  # Output file path (relative paths are relative to repo root).
  output_video: "outputs/output_test.mp4"
  # Phase 2: style control at inference time.
  # - "random": sample a new style latent per run (default)
  # - "zero": disable style randomness (useful for smoke tests / audio-reactivity checks)
  style_mode: "random"
  # Optional integer seed for deterministic style sampling (null = no seeding).
  style_seed: null
  # Phase 5: classifier-free guidance for audio conditioning.
  # 1.0 = no guidance; 1.5–4.0 can make audio effects more obvious (at some cost to diversity).
  audio_guidance_scale: 1.0

# Phase 3 data diversity knobs (optional).
# These affect which procedural visualizers are sampled for synthetic training data.
visualizers:
  # Probability of using an augmented wrapper (smooth postprocess stack).
  aug_prob: 0.35
  # Probability of choosing an organic/painterly family vs geometric.
  organic_prob: 0.5
  # Probability of composing 2–3 primitives into a single clip.
  composite_prob: 0.25
